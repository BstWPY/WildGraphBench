#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å¼•ç”¨æå–è„šæœ¬

åŠŸèƒ½:
1. ä½¿ç”¨ jina_scraping ä¸­çš„ WebScrapingJinaTool æŠ“å–ç½‘é¡µå¹¶ä¿å­˜ Markdownï¼ˆå½¢æˆ ç›®å½•/æ ‡é¢˜/reference/ ç»“æ„ï¼‰
2. ä»ä¿å­˜çš„ Markdown ä¸­è§£æ References æ®µè½é‡Œçš„å¼•ç”¨é“¾æ¥
3. ç”Ÿæˆ reference/references.jsonlï¼Œæ¯è¡Œä¸€ä¸ªå¼•ç”¨é¡¹ï¼ŒåŒ…å«ï¼š
   - url
   - is_external (æ˜¯å¦æ˜¯å¤–é“¾)
   - jumpup (è‹¥åŒ…å«è„šæ³¨"â†‘"æˆ–"jump up"ç­‰æ ‡è®°åˆ™ç»™å‡ºå¯¹è±¡ä¿¡æ¯ï¼Œå¦åˆ™ä¸ºç©ºå­—ç¬¦ä¸²)
   - title (å¼•ç”¨æ ‡é¢˜ä¼°è®¡)
   - date (è‹¥èƒ½è§£æå‡ºæ—¥æœŸ)

è¯´æ˜:
ç”±äº Jina æŠ“å–è¿”å›çš„æ˜¯ç»è¿‡æç‚¼çš„æ–‡æœ¬è€ŒéåŸå§‹ HTMLï¼Œæœ¬è„šæœ¬é‡‡ç”¨å¯å‘å¼è§£æï¼Œ
å¯èƒ½ä¸èƒ½ 100% å¤åŸå¤æ‚çš„ Wikipedia å¼•ç”¨ç»“æ„ï¼›å¯æŒ‰éœ€åç»­æ”¹ä¸ºç›´æ¥ç”¨ MediaWiki API è·å–æ›´ç»“æ„åŒ–æ•°æ®ã€‚
"""

from __future__ import annotations

import argparse
import json
import os
import re
import time
from typing import List, Dict, Any, Optional

from jina_scraping import WebScrapingJinaTool, save_markdown, DEFAULT_API_KEY, slugify  # type: ignore
from urllib.parse import urlparse, unquote

# é‡è¯•é…ç½®
MAX_RETRIES = 10       # æœ€å¤šé‡è¯•10æ¬¡

# URL æå–æ­£åˆ™ï¼šåŒ¹é… http/https å¼€å¤´ç›´åˆ°é‡åˆ°ç©ºç™½æˆ–å³æ‹¬å·/å¼•å·/æ–¹æ‹¬å·
URL_REGEX = re.compile(r'(https?://[^\s\)\]\><"\']+)')
DATE_ISO_REGEX = re.compile(r'\b(\d{4}-\d{2}-\d{2})\b')
DATE_YMD_REGEX = re.compile(r'\b(\d{4})[\./å¹´-](\d{1,2})[\./æœˆ-](\d{1,2})æ—¥?\b')
MONTH_NAME_REGEX = re.compile(
    r'\b(January|February|March|April|May|June|July|August|September|October|November|December)\s+\d{1,2},\s+\d{4}\b'
)
RETRIEVED_DATE_REGEX = re.compile(
    r'(Retrieved\s+(January|February|March|April|May|June|July|August|September|October|November|December)\s+\d{1,2},\s+\d{4})\.?',
    re.IGNORECASE
)

# Jump/è„šæ³¨æ¨¡å¼ï¼ˆçª„åŒ–ï¼šä»…å»é™¤å•ä¸ª **[^](...)** æˆ– ^ åŠç´§éšçš„ anchor é“¾æ¥ï¼Œè€Œä¸åæ‰åç»­æ­£æ–‡ï¼‰
JUMP_CARET_PATTERN = re.compile(r'(\*\*\[\^]\([^)]*\)\*\*)')  # **[^](...)**
JUMP_INLINE_ANCHORS_PATTERN = re.compile(
    r'(\[[^\]]+\]\(https?://[^)]+cite_ref[^)]*\))'
)  # a,b,cé”šç‚¹
JUMP_PHRASE_PATTERN = re.compile(r'^\s*\^?\s*Jump up to:?', re.IGNORECASE)
MD_LINK_REGEX = re.compile(r'\[([^\]]+)\]\((https?://[^)]+)\)')

def url_to_slug_title(url: str) -> str:
    """ä» Wiki URL æ¨å¯¼ä¸€ä¸ªâ€œæ ‡é¢˜å­—ç¬¦ä¸²â€ï¼Œä¾›ç›®å½•å‘½åç”¨ã€‚

    ä¾‹å¦‚:
        https://en.wikipedia.org/wiki/Al_Jazeera_Media_Network
        -> 'Al Jazeera Media Network'
    """
    path = urlparse(url).path
    last = path.rsplit('/', 1)[-1]  # å–æœ€åä¸€æ®µ
    last = unquote(last)            # å¤„ç† %20 ç­‰
    title = last.replace('_', ' ')
    return title or 'page'

def scrape_with_retry(scraper, url: str) -> Optional[Dict]:
  """å¸¦é‡è¯•æœºåˆ¶çš„ç½‘é¡µæŠ“å–

  Args:
      scraper: WebScrapingJinaTool å®ä¾‹
      url: è¦æŠ“å–çš„URL

  Returns:
      æŠ“å–ç»“æœæˆ–None
  """
  for attempt in range(MAX_RETRIES):
    try:
      print(f"ğŸ”„ å°è¯•æŠ“å– (ç¬¬ {attempt + 1}/{MAX_RETRIES} æ¬¡): {url}")

      # è°ƒç”¨æŠ“å–å™¨ï¼Œä¸ä¼ å…¥timeoutå‚æ•°
      data = scraper(url)

      if data and data.get('content'):
        content_length = len(data.get('content', ''))
        print(f"âœ… æŠ“å–æˆåŠŸ: {content_length} å­—ç¬¦")
        return data
      else:
        print(f"âš ï¸  æŠ“å–è¿”å›ç©ºå†…å®¹")
    except Exception as e:
      error_msg = str(e)
      print(f"âŒ æŠ“å–å¤±è´¥ (å°è¯• {attempt + 1}): {error_msg}")

      if attempt < MAX_RETRIES - 1:
        # ç®€å•çš„ç­‰å¾…ç­–ç•¥
        wait_time = (attempt + 1) * 10  # 10ç§’é€’å¢ç­‰å¾…
        print(f"â³ ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
        time.sleep(wait_time)
      else:
        print(f"ğŸš« æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†")

  return None


def safe_url_to_title(url: str) -> str:
  """ä»URLå®‰å…¨åœ°æå–æ ‡é¢˜ï¼Œå¤„ç†ç‰¹æ®Šå­—ç¬¦"""
  try:
    import urllib.parse

    # è§£ç URL
    decoded_url = urllib.parse.unquote(url)
    # æå–æœ€åä¸€éƒ¨åˆ†ä½œä¸ºæ ‡é¢˜
    title = decoded_url.split('/')[-1]
    # æ›¿æ¢ä¸‹åˆ’çº¿ä¸ºç©ºæ ¼
    title = title.replace('_', ' ')
    # ç§»é™¤ä¸å®‰å…¨çš„æ–‡ä»¶åå­—ç¬¦
    title = re.sub(r'[<>:"/\\|?*]', '', title)
    return title.strip()
  except Exception as e:
    print(f"âš ï¸  URLæ ‡é¢˜æå–å¤±è´¥ {url}: {e}")
    return "Unknown_Title"


def create_error_placeholder(url: str, output_dir: str, error_msg: str) -> tuple[str, str]:
  """åˆ›å»ºé”™è¯¯å ä½ç¬¦æ–‡ä»¶

  Args:
      url: å¤±è´¥çš„URL
      output_dir: è¾“å‡ºç›®å½•
      error_msg: é”™è¯¯ä¿¡æ¯

  Returns:
      (markdown_path, jsonl_path) å…ƒç»„
  """
  title = safe_url_to_title(url)
  error_dir = os.path.join(output_dir, "Error")
  os.makedirs(error_dir, exist_ok=True)

  # åˆ›å»ºé”™è¯¯Markdownæ–‡ä»¶
  error_md_path = os.path.join(error_dir, "Error.md")
  with open(error_md_path, 'w', encoding='utf-8') as f:
    f.write("# Error\n\n")
    f.write(f"æŠ“å–å¤±è´¥: {url}\n\n")
    f.write(f"åŸå› : {error_msg}\n\n")
    f.write("å»ºè®®: æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•\n")

  # åˆ›å»ºç©ºçš„å¼•ç”¨æ–‡ä»¶
  reference_dir = os.path.join(error_dir, 'reference')
  os.makedirs(reference_dir, exist_ok=True)
  jsonl_path = os.path.join(reference_dir, 'references.jsonl')

  with open(jsonl_path, 'w', encoding='utf-8') as f:
    # åˆ›å»ºç©ºæ–‡ä»¶
    pass

  return error_md_path, jsonl_path


def parse_references_block(markdown_text: str) -> List[str]:
  """å®šä½å‚è€ƒæ–‡çŒ®åŒºå—å’ŒBibliographyåŒºå—å¹¶è¿”å›å…¶ä¸­çš„åŸå§‹è¡Œã€‚

  æ”¯æŒä»¥ä¸‹ç»“æ„ç¤ºä¾‹:
  1. References\n----------\n### Citations\n<entries>
  2. # References / ## References å½¢å¼
  3. Bibliography / ## Bibliography å½¢å¼
  4. é¡µé¢æœ«å°¾çš„æ— æ ‡é¢˜å¼•ç”¨åˆ—è¡¨ï¼ˆTell es-Sakanæ¨¡å¼ï¼‰
  5. ç¼–å·å¼•ç”¨åˆ—è¡¨ï¼ˆ1. ^ Jump up to: ... æ¨¡å¼ï¼‰
  """
  lines = markdown_text.splitlines()
  n = len(lines)
  ref_start_idx: Optional[int] = None
  bib_start_idx: Optional[int] = None
  citations_anchor_idx: Optional[int] = None
  numbered_refs_start: Optional[int] = None

  for i, ln in enumerate(lines):
    stripped = ln.strip()
    low = stripped.lower()

    # æŸ¥æ‰¾ References éƒ¨åˆ†
    if low in {"references", "å‚è€ƒæ–‡çŒ®"}:
      ref_start_idx = i
      if i + 1 < n and re.fullmatch(r'-{3,}', lines[i + 1].strip()):
        pass
    # ç›´æ¥ markdown å½¢å¼æ ‡é¢˜ - References
    if low.startswith('#') and 'references' in low:
      ref_start_idx = i

    # æŸ¥æ‰¾ Bibliography éƒ¨åˆ†
    if low in {"bibliography", "ä¹¦ç›®", "å‚è€ƒä¹¦ç›®"}:
      bib_start_idx = i
      if i + 1 < n and re.fullmatch(r'-{3,}', lines[i + 1].strip()):
        pass
    # ç›´æ¥ markdown å½¢å¼æ ‡é¢˜ - Bibliography
    if low.startswith('#') and 'bibliography' in low:
      bib_start_idx = i

    if '### citations' in low:
      citations_anchor_idx = i
      if ref_start_idx is None:
        ref_start_idx = i

    # æŸ¥æ‰¾ç¼–å·å¼•ç”¨åˆ—è¡¨çš„å¼€å§‹ï¼ˆå¦‚ "1. ^ Jump up to:" æˆ– "1. **^**"ï¼‰
    if numbered_refs_start is None and re.match(
      r'^\s*1\.\s*[\^\*]*\s*(Jump up|[\*\^])', stripped, re.IGNORECASE
    ):
      numbered_refs_start = i

  # æ”¶é›†æ‰€æœ‰ç›¸å…³å†…å®¹
  collected: List[str] = []

  # å¤„ç† References éƒ¨åˆ†
  if ref_start_idx is not None:
    start_collect = (citations_anchor_idx + 1) if citations_anchor_idx is not None else (ref_start_idx + 1)
    end_collect = min(
      x
      for x in [bib_start_idx, numbered_refs_start, n]
      if x is not None and x > ref_start_idx
    )

    for j in range(start_collect, end_collect):
      if j >= n:
        break
      ln = lines[j]
      stripped = ln.strip()
      if re.match(r'^#{1,3} ', stripped):
        low = stripped.lower()
        if not ('reference' in low or 'citation' in low or 'bibliography' in low):
          break
      if stripped.lower().startswith(('external links', 'see also', 'notes')):
        if 'bibliography' not in stripped.lower():
          break
      collected.append(ln)

  # å¤„ç† Bibliography éƒ¨åˆ†
  if bib_start_idx is not None:
    start_collect = bib_start_idx + 1
    end_collect = (
      numbered_refs_start if numbered_refs_start and numbered_refs_start > bib_start_idx else n
    )

    for j in range(start_collect, end_collect):
      if j >= n:
        break
      ln = lines[j]
      stripped = ln.strip()
      if re.match(r'^#{1,3} ', stripped):
        low = stripped.lower()
        if not ('bibliography' in low or 'reference' in low or 'citation' in low):
          break
      if stripped.lower().startswith(('external links', 'see also', 'notes')):
        break
      collected.append(ln)

  # å¤„ç†ç¼–å·å¼•ç”¨åˆ—è¡¨
  if numbered_refs_start is not None:
    for j in range(numbered_refs_start, n):
      ln = lines[j]
      stripped = ln.strip()

      # å¦‚æœæ˜¯ç¼–å·å¼•ç”¨æ ¼å¼ï¼Œç»§ç»­æ”¶é›†
      if re.match(r'^\s*\d+\.\s*[\^\*]*\s*(Jump up|\*)', stripped, re.IGNORECASE):
        collected.append(ln)
      # å¦‚æœæ˜¯ç©ºè¡Œï¼Œè·³è¿‡
      elif not stripped:
        collected.append(ln)
      # å¦‚æœé‡åˆ°æ–°çš„ç« èŠ‚æ ‡é¢˜ï¼Œåœæ­¢
      elif re.match(r'^#{1,3} ', stripped):
        break
      # å¦‚æœé‡åˆ°æ˜æ˜¾çš„å…¶ä»–å†…å®¹ï¼Œåœæ­¢
      elif stripped.lower().startswith(('external links', 'see also', 'notes')):
        break
      # å¦åˆ™è®¤ä¸ºæ˜¯å¼•ç”¨çš„å»¶ç»­éƒ¨åˆ†
      else:
        collected.append(ln)

  # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ­£å¼çš„References/Bibliographyæ®µï¼Œä¹Ÿæ²¡æœ‰ç¼–å·å¼•ç”¨ï¼ŒæŸ¥æ‰¾é¡µé¢æœ«å°¾çš„å¼•ç”¨åˆ—è¡¨
  if ref_start_idx is None and bib_start_idx is None and numbered_refs_start is None:
    print("ğŸ” æœªæ‰¾åˆ°æ ‡å‡†å¼•ç”¨æ ¼å¼ï¼Œå°è¯•å¤‡ç”¨ç­–ç•¥...")
    # åŸæœ‰çš„æœ«å°¾å¼•ç”¨æ£€æµ‹é€»è¾‘
    ref_lines_from_end = []
    for i in range(n - 1, -1, -1):
      line = lines[i].strip()
      if not line:
        continue
      if line.startswith('*') and '[' in line and '](' in line:
        ref_lines_from_end.insert(0, lines[i])
      elif 'wikimedia' in line.lower() and '[' in line:
        ref_lines_from_end.insert(0, lines[i])
      elif line.startswith('#') or 'edit section' in line.lower():
        break
      elif (
        not line.startswith('*')
        and '[' not in line
        and len(line) > 20
      ):
        break

    if ref_lines_from_end:
      print(f"ğŸ“‹ å¤‡ç”¨ç­–ç•¥æ‰¾åˆ° {len(ref_lines_from_end)} è¡Œå¯èƒ½çš„å¼•ç”¨")
      collected.extend(ref_lines_from_end)

    print(f"ğŸ“Š è§£æç»“æœ: æ”¶é›†åˆ° {len(collected)} è¡Œå¼•ç”¨å†…å®¹")

  return collected


def group_reference_entries(ref_lines: List[str]) -> List[str]:
  """å°†å¼•ç”¨åŒºå—æŒ‰ç©ºè¡Œæˆ–ç¼–å·/åˆ—è¡¨èµ·å§‹åˆ†ç»„ï¼Œè¾“å‡ºæ¯æ¡å¼•ç”¨æ–‡æœ¬ã€‚"""
  entries: List[str] = []
  buffer: List[str] = []

  def flush():
    if buffer:
      # åˆå¹¶å¹¶å‹ç¼©ç©ºç™½
      merged = ' '.join(l.strip() for l in buffer if l.strip())
      if merged:
        entries.append(merged)
      buffer.clear()

  for ln in ref_lines:
    stripped = ln.strip()
    if not stripped:
      flush()
      continue
    # å…¸å‹ç¼–å·å½¢å¼ [1] æˆ– 1. æˆ– - å·å¼€å¤´ï¼Œæ–°æ¡ç›®
    if re.match(r'^\s*(\[[0-9]+\]|[0-9]+[.)]|[-*])\s+', ln):
      flush()
      buffer.append(stripped)
    else:
      buffer.append(stripped)
  flush()
  return entries


def extract_date(text: str) -> Optional[str]:
  # ä¼˜å…ˆåŒ¹é… Retrieved æ ¼å¼ (åŒ…å«å°¾éƒ¨å¥ç‚¹)
  m = RETRIEVED_DATE_REGEX.search(text)
  if m:
    phrase = m.group(1)
    # ç¡®ä¿ä»¥å¥ç‚¹ç»“æŸ
    return phrase if phrase.endswith('.') else phrase + '.'
  # ISO
  m = DATE_ISO_REGEX.search(text)
  if m:
    return m.group(1)
  # YYYY-MM-DD æˆ–å¸¦ä¸­æ–‡åˆ†éš”
  m = DATE_YMD_REGEX.search(text)
  if m:
    y, mo, d = m.groups()
    return f"{int(y):04d}-{int(mo):02d}-{int(d):02d}"
  # Month name (å‡ºç‰ˆæ—¥æœŸ) ç›´æ¥è¿”å›
  m = MONTH_NAME_REGEX.search(text)
  if m:
    return m.group(0)
  return None


def build_reference_items(ref_entries: List[str]) -> List[Dict[str, Any]]:
  """è§£æå¼•ç”¨æ¡ç›® -> ç»“æ„åŒ–å­—æ®µã€‚

  æ–°è§„åˆ™æ±‡æ€»ï¼ˆæ¥è‡ª ref_cases.mdï¼‰ï¼š

   1. å»é™¤æ‰€æœ‰ "Jump up" / a b c è„šæ³¨è·³è½¬é”šç‚¹ (åŒ…å« cite_ref / cite_note / caret ^ å½¢å¼)ã€‚
   2. è‹¥é¦–ä¸ª"æ­£æ–‡"é“¾æ¥æ˜¯åŒé¡µé”šç‚¹ (#CITEREF / #cite_ref) ä¸”æ•´æ¡ä¸­ä¸å­˜åœ¨ä»»ä½•å¤–é“¾æˆ– Archive é“¾æ¥ï¼Œ
      åˆ™æ•´æ¡ä¸¢å¼ƒ (çº¯é¡µå†…ä¹¦ç›®/Works cited æŒ‡å‘)ã€‚
   3. æ ‡é¢˜ = ç¬¬ä¸€æ¡ *å†…å®¹* é“¾æ¥æ–‡æœ¬ (å‰¥ç¦»å¼•å· / å‰åç©ºç™½)ã€‚
       - è‹¥è¯¥é“¾æ¥ URL ä»æ˜¯ wikipedia.org ä¸”å­˜åœ¨ [Archived](archive_url) é“¾æ¥ï¼Œåˆ™æ ‡é¢˜ä»å–ç¬¬ä¸€æ¡ï¼Œæœ€ç»ˆ url å– Archived é“¾æ¥ã€‚
   4. è‹¥å­˜åœ¨ [Archived](archive_url)ï¼Œä¿ç•™ archive_url å­—æ®µï¼Œä½†æœ€ç»ˆæŠ“å–ä½¿ç”¨åŸå§‹é Archived é“¾æ¥ (url å­—æ®µ)ï¼›ä¸å†æ›¿æ¢ä¸ºå½’æ¡£åœ°å€ã€‚
   5. ä½œè€…ï¼š
       - æœ‰ (Month Day, Year) å‡ºç‰ˆæ—¥æœŸï¼šæ—¥æœŸå‰çš„éç©ºæ–‡æœ¬ï¼ˆå»é™¤è·³è½¬é”šç‚¹ä¸å¤šä½™æ ‡ç‚¹ï¼‰ã€‚
       - æ— å‡ºç‰ˆæ—¥æœŸï¼šè‹¥åœ¨ç¬¬ä¸€æ¡æ ‡é¢˜é“¾æ¥ä¹‹å‰å‡ºç°ä»¥å¥ç‚¹ç»“æŸçš„çŸ­æ–‡æœ¬ (<= 12 è¯)ï¼Œè§†ä¸ºä½œè€…ã€‚ä¾‹å¦‚ï¼š"United States Congress."ã€‚
   6. sourcesï¼šæ”¶é›†æ ‡é¢˜ä¹‹åçš„æ‰€æœ‰é 'Archived' é“¾æ¥æ–‡æœ¬ï¼›åŒ…æ‹¬åª’ä½“åã€å‡ºç‰ˆç‰©ã€ISSN/ISBN åŠå…¶ç¼–å·é“¾æ¥ï¼›ä¿æŒå»é‡é¡ºåºã€‚
      è‹¥åªæœ‰ 1 ä¸ª source ä»æ”¾åœ¨åˆ—è¡¨é‡Œã€‚
   7. è¿‡æ»¤ï¼š
       - æ²¡æœ‰ä»»ä½•å¯æŠ“å– urlï¼ˆæ—¢æ— å¤–éƒ¨ http(s) é“¾æ¥ï¼Œäº¦æ— å½’æ¡£é“¾æ¥ï¼‰=> ä¸¢å¼ƒ (å¦‚çº¯ä¹¦ç›®ï¼š_Promises to Keep: ..._ æ— é“¾æ¥)ã€‚
       - ä»…å«å†…éƒ¨ #CITEREF/#cite_ref é“¾æ¥ => ä¸¢å¼ƒã€‚
   8. å½’ä¸€åŒ–ï¼š
       - æ ‡é¢˜å»é™¤é¦–å°¾ä¸­æ–‡/è‹±æ–‡å¼•å·ã€å¼ºè°ƒç¬¦å· _ *ã€‚
       - å»æ‰é‡å¤ç©ºç™½ã€‚
   9. æ—¥æœŸï¼š
       - publish_dateï¼šç¬¬ä¸€ä¸ª (Month Day, Year) æ ·å¼ã€‚
       - retrieved_dateï¼š'Retrieved Month Day, Year'ã€‚
  10. ä»ä¿ç•™ is_externalï¼šæŒ‡æœ€ç»ˆä½¿ç”¨çš„ url æ˜¯å¦å¤–éƒ¨ (é wikipedia.org)ã€‚
  11. ä½œè€…ä¸ºç©ºæ—¶ï¼Œè‹¥ sources å­˜åœ¨ï¼Œä½œè€…å›å¡«ä¸ºç¬¬ä¸€ä¸ª sourceï¼ˆæœºæ„å³ä½œè€…ï¼‰ã€‚
  """
  items: List[Dict[str, Any]] = []
  month_names = (
    r'(January|February|March|April|May|June|July|August|September|October|November|December)'
  )
  publish_date_pat = re.compile(r'\((' + month_names + r')\s+\d{1,2},\s+\d{4}\)')
  retrieved_pat = re.compile(
    r'Retrieved\s+' + month_names + r'\s+\d{1,2},\s+\d{4}\.?',
    re.IGNORECASE
  )

  def is_jump_token(text: str, url: str) -> bool:
    t = text.lower().strip('_* \'"')
    if 'jump' in t:  # Jump up / Jump up to
      return True
    # cite_ref anchors + single-letter label (a,b,c, etc.)
    if 'cite_ref' in url and re.fullmatch(r'[a-z]', t):
      return True
    # caret marker
    if t in {'^'}:
      return True
    return False

  for raw_entry in ref_entries:
    entry = raw_entry.strip()
    if not entry:
      continue
    # å»é™¤è¡Œé¦–ç¼–å· "1."ã€"[1]"ã€"-" ç­‰
    entry = re.sub(r'^\s*(\[[0-9]+\]|[0-9]+[.)]|[-*])\s+', '', entry)
    # å»é™¤å¼€å¤´ Jump up phrase
    entry = JUMP_PHRASE_PATTERN.sub('', entry)
    # å»é™¤ caret jump ç»„ä»¶ **[^](...)**
    entry = JUMP_CARET_PATTERN.sub(' ', entry)
    # å»é™¤ cite_ref é”šç‚¹é“¾æ¥é›†åˆ (a,b,c...)
    entry = JUMP_INLINE_ANCHORS_PATTERN.sub(' ', entry)
    entry = re.sub(r'\s+', ' ', entry).strip()

    md_links = MD_LINK_REGEX.findall(entry)
    if not md_links:
      # æ²¡æœ‰ä»»ä½•é“¾æ¥ => å¯èƒ½æ˜¯ä¹¦ç±ï¼ˆæ—  urlï¼‰ -> ä¸¢å¼ƒ
      continue

    # è¿‡æ»¤æ‰çº¯è„šæ³¨/è·³è½¬/å•å­—æ¯é”šç‚¹
    def is_pure_anchor(txt: str, url: str) -> bool:
      t = txt.strip().lower().strip('_*"')
      if not url:
        return True
      if 'cite_ref' in url or 'cite_note' in url:
        return True
      if re.match(r'^[a-z]$', t):
        return True
      if 'jump' in t:
        return True
      return False

    content_links = [(t, u) for (t, u) in md_links if not is_pure_anchor(t, u)]
    if not content_links:
      continue

    # è¯†åˆ«å‡ºç‰ˆæ—¥æœŸ & Retrieved
    m_pub = publish_date_pat.search(entry)
    m_ret = retrieved_pat.search(entry)
    publish_date = m_pub.group(0).strip('()') if m_pub else ''
    retrieved_date = m_ret.group(0) if m_ret else ''
    if retrieved_date and not retrieved_date.endswith('.'):
      retrieved_date += '.'

    def clean_name(s: str) -> str:
      s = re.sub(r'\s+', ' ', s).strip()
      # å»æ‰å¼€å¤´ç¬¦å· ^ * _ ä»¥åŠå¤šä½™æ ‡ç‚¹
      s = re.sub(r'^[\^*_\s]+', '', s)
      s = s.strip()
      # é¿å…äº§ç”Ÿç©ºå­—ç¬¦ä¸²
      return s

    # ä½œè€…å€™é€‰åŒºåŸŸ
    author_segment = ''
    if m_pub:
      author_segment = entry[:m_pub.start()].strip()
    else:
      # æ— å‡ºç‰ˆæ—¥æœŸï¼šæˆªå–åˆ°ç¬¬ä¸€ä¸ªå¤–éƒ¨(é wikipedia)é“¾æ¥æˆ–ç¬¬ä¸€ä¸ªå¼•å·åŒ…è£¹æ ‡é¢˜é“¾æ¥å‰
      first_ext_idx = None
      for t, u in content_links:
        if 'wikipedia.org' not in u:
          # ä½ç½®åŸºäºå…¨æ–‡æœç´¢
          pos = entry.find('[' + t + '](')
          if pos != -1:
            first_ext_idx = pos
            break
      if first_ext_idx is not None:
        author_segment = entry[:first_ext_idx].strip()
      else:
        # è‹¥æ²¡æœ‰å¤–éƒ¨é“¾æ¥ï¼Œä»¥ç¬¬ä¸€ä¸ªé“¾æ¥å‰ä¸ºä½œè€…
        first_link_text = content_links[0][0]
        pos = entry.find('[' + first_link_text + '](')
        author_segment = entry[:pos].strip() if pos != -1 else ''

    # æ¸…é™¤ä½œè€…æ®µä¸­çš„æ®‹ç•™ jump/é”šç‚¹é“¾æ¥
    if author_segment:
      author_segment = JUMP_CARET_PATTERN.sub(' ', author_segment)
      author_segment = JUMP_INLINE_ANCHORS_PATTERN.sub(' ', author_segment)
      author_segment = re.sub(
        r'(\[[^\]]+\]\(https?://[^)]+\))',
        lambda m: re.sub(r'^\[|\]\([^)]*\)$', '', m.group(0)),
        author_segment
      )
      author_segment = re.sub(
        r'\[[^\]]+\]\(https?://[^)]+\)',
        lambda m: re.sub(r'^\[|\]\([^)]*\)$', '', m.group(0)),
        author_segment
      )
      author_segment = re.sub(r'\s+', ' ', author_segment).strip()
      # å»æ‰æœ«å°¾å¥ç‚¹
      if author_segment.endswith('.'):
        author_segment = author_segment[:-1].strip()

    author = author_segment
    # è‹¥ä½œè€…è¿‡é•¿ (> 15 è¯) è§†ä¸ºå™ªå£°æ”¾å¼ƒ
    if author and len(author.split()) > 15:
      author = ''

    # ç¡®å®šæ ‡é¢˜é“¾æ¥ï¼šä¼˜å…ˆç­–ç•¥
    title_link = None
    # 1. åœ¨å‡ºç‰ˆæ—¥æœŸä¹‹åçš„é“¾æ¥ä¸­ï¼Œä¼˜å…ˆé€‰æ‹©æ–‡æœ¬è¢«å¼•å·åŒ…è£¹ä¸”ä¸ºå¤–éƒ¨é“¾æ¥
    after_pub_pos = m_pub.end() if m_pub else (len(author_segment) if author_segment else 0)
    for t, u in content_links:
      pos = entry.find('[' + t + '](')
      if pos < after_pub_pos:
        continue
      txt = t.strip()
      if ('wikipedia.org' not in u) and re.match(r'^".*"$|^".*"$', txt):
        title_link = (t, u)
        break
    # 2. å¤–éƒ¨é“¾æ¥ï¼ˆæ—¥æœŸä¹‹åï¼‰
    if not title_link:
      for t, u in content_links:
        pos = entry.find('[' + t + '](')
        if pos < after_pub_pos:
          continue
        if 'wikipedia.org' not in u:
          title_link = (t, u)
          break
    # 3. ä»»ä½•ï¼ˆæ—¥æœŸä¹‹åï¼‰
    if not title_link:
      for t, u in content_links:
        pos = entry.find('[' + t + '](')
        if pos >= after_pub_pos:
          title_link = (t, u)
          break
    # 4. å›é€€ï¼šç¬¬ä¸€ä¸ªä¸åœ¨ä½œè€…æ®µä¸­çš„é“¾æ¥
    if not title_link:
      for t, u in content_links:
        pos = entry.find('[' + t + '](')
        if not author_segment or pos >= len(author_segment):
          title_link = (t, u)
          break
    if not title_link:
      continue
    title_text, title_url = title_link

    # æŸ¥æ‰¾ Archived é“¾æ¥ï¼ˆè®°å½•ä½†ä¸ç”¨ä½œä¸» urlï¼‰
    archive_url = None
    for t, u in content_links:
      if t.lower() == 'archived':
        archive_url = u
        break

    # æå– sourceï¼šæ ‡é¢˜é“¾æ¥ä¹‹åç¬¬ä¸€ä¸ªé Archivedã€éåŒ URL çš„é“¾æ¥æ–‡æœ¬
    title_pos = entry.find('[' + title_text + '](')
    source = ''
    for t, u in content_links:
      if t == title_text and u == title_url:
        continue
      pos = entry.find('[' + t + '](')
      if pos < title_pos:
        continue
      if t.lower() == 'archived':
        continue
      if t.strip() == title_text.strip():
        continue
      source = t.strip('_* ')
      break

    # è‹¥æœªæ‰¾åˆ°æ¥æºæˆ–æ¥æºä¸ä½œè€…ç›¸åŒï¼Œå°è¯•è§£ææ ‡é¢˜åçº¯æ–‡æœ¬åª’ä½“åç§°
    if not source or source == author:
      link_markdown = f'[{title_text}]({title_url})'
      link_end = entry.find(link_markdown)
      if link_end != -1:
        link_end += len(link_markdown)
        tail = entry[link_end:]
        # æˆªæ–­åˆ° Archived / Retrieved ä¹‹å‰
        cut_idx = len(tail)
        for kw in ['Archived', 'Retrieved']:
          kpos = tail.find(kw)
          if kpos != -1 and kpos < cut_idx:
            cut_idx = kpos
        tail_section = tail[:cut_idx]
        # æ–œä½“åª’ä½“ _..._
        m_italic = re.search(r'_(\s*[^_]{2,}?)_', tail_section)
        candidate = ''
        if m_italic:
          candidate = m_italic.group(1).strip()
        else:
          # é¦–ä¸ªä»¥å¥ç‚¹ç»“æŸçš„è¿ç»­å¤§å†™/é¦–å­—æ¯å¤§å†™è¯ç»„ (NPR. / Associated Press.)
          m_acro = re.match(
            r'\s*([A-Z][A-Za-z&\.]*?(?:\s+[A-Z][A-Za-z&\.]*?){0,4})\.(?:\s|$)',
            tail_section
          )
          if m_acro:
            cand = m_acro.group(1).strip()
            # è¿‡æ»¤ Retrieved / Archived è¯¯åˆ¤
            if cand.lower() not in {'retrieved', 'archived'}:
              candidate = cand
        if not candidate:
          # æ•è·å¦‚ "NPR." å‡ºç°åœ¨æ ‡é¢˜é“¾æ¥å
          m_npr = re.search(r'\b(NPR)\.(?:\s|$)', tail_section)
          if m_npr:
            candidate = m_npr.group(1)
        if candidate and candidate != author:
          source = candidate.strip('_* ')

    # è¿‡æ»¤å†…éƒ¨ works citedï¼šè‹¥æ ‡é¢˜ä»æ˜¯ wikipedia é“¾æ¥ä¸”æ²¡æœ‰å¤–éƒ¨é“¾æ¥
    if 'wikipedia.org' in title_url and not any(
      'wikipedia.org' not in u for _, u in content_links
    ):
      continue

    # æ¸…æ´—æ ‡é¢˜
    title = title_text.strip().strip('"""').strip('_* ')
    title = re.sub(r'\s+', ' ', title)

    # ä½œè€…/æ¥æºäº’è¡¥
    if not author and source:
      author = source
    if not source and author:
      source = author

    author = clean_name(author)
    source = clean_name(source)
    # è‹¥æ¸…æ´—åä»ä¸ºç©ºä¸”å½¼æ­¤å­˜åœ¨äº’è¡¥
    if not author and source:
      author = source
    if not source and author:
      source = author

    is_external = ('wikipedia.org' not in title_url)
    item: Dict[str, Any] = {
      'title': title,
      'url': title_url,
      'is_external': is_external,
    }
    if author:
      item['author'] = author
    if source:
      item['source'] = source
    if archive_url:
      item['archive_url'] = archive_url
    if publish_date:
      item['publish_date'] = publish_date
    if retrieved_date:
      item['retrieved_date'] = retrieved_date
    # scraped æ ‡å¿—åœ¨å†™å‡ºæ—¶è¡¥
    items.append(item)

  return items


def write_jsonl(items: List[Dict[str, Any]], path: str) -> None:
  os.makedirs(os.path.dirname(path), exist_ok=True)
  with open(path, 'w', encoding='utf-8') as f:
    for it in items:
      # åˆå§‹åŒ– scraped æ ‡è®°
      if 'scraped' not in it:
        it['scraped'] = False
      f.write(json.dumps(it, ensure_ascii=False) + '\n')


def main():
  parser = argparse.ArgumentParser(description='æŠ“å–ç½‘é¡µå¹¶æå–å‚è€ƒæ–‡çŒ®ç”Ÿæˆ JSONL')
  parser.add_argument('--url', required=True, help='ç›®æ ‡ç½‘é¡µ URL (åŸºå‡† URL)')
  parser.add_argument('--output_dir', required=True, help='è¾“å‡ºæ ¹ç›®å½•')
  parser.add_argument('--api-key', dest='api_key', default=None, help='Jina API Key (å¯é€‰)')
  args = parser.parse_args()

  print(f"ğŸŒ å¼€å§‹å¤„ç† URL: {args.url}")
  print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
  print(f"ğŸ”„ é‡è¯•æ¬¡æ•°: {MAX_RETRIES}")

  api_key = args.api_key or os.environ.get('JINA_API_KEY') or DEFAULT_API_KEY
  if not api_key.startswith('Bearer '):
    api_key = f'Bearer {api_key}'

  scraper = WebScrapingJinaTool(api_key)
  # ç”¨ URL æ¨å¯¼ä¸€ä¸ªâ€œç¨³å®šæ ‡é¢˜â€ï¼Œä¿è¯ç›®å½•å‘½åä¸ run_ref_scraper ä¸€è‡´
  url_title = url_to_slug_title(args.url)

  # ä½¿ç”¨å¸¦é‡è¯•çš„æŠ“å–
  data = scrape_with_retry(scraper, args.url)

  if not data or not data.get('content'):
    print("âŒ æŠ“å–å¤±è´¥ï¼Œæ— æ³•è·å–é¡µé¢å†…å®¹")

    # åˆ›å»ºé”™è¯¯å ä½ç¬¦
    error_md_path, jsonl_path = create_error_placeholder(
      args.url,
      args.output_dir,
      "ç½‘ç»œè¶…æ—¶æˆ–æœåŠ¡ä¸å¯ç”¨"
    )

    print(f"ğŸ“„ é”™è¯¯é¡µé¢: {error_md_path}")
    print(f"ğŸ“‹ ç©ºå¼•ç”¨æ–‡ä»¶: {jsonl_path}")
    return

  # ä¿å­˜ Markdown
  try:
    md_path = save_markdown(data, args.output_dir, slug=url_title)
    print(f"ğŸ“„ Markdown å·²ä¿å­˜: {md_path}")
  except Exception as e:
    print(f"âŒ ä¿å­˜ Markdown å¤±è´¥: {e}")
    return

  # è¯»å– markdown å†…å®¹
  try:
    with open(md_path, 'r', encoding='utf-8') as f:
      markdown_text = f.read()
  except Exception as e:
    print(f"âŒ è¯»å– Markdown æ–‡ä»¶å¤±è´¥: {e}")
    return

  reference_dir = os.path.join(os.path.dirname(md_path), 'reference')
  os.makedirs(reference_dir, exist_ok=True)
  jsonl_path = os.path.join(reference_dir, 'references.jsonl')

  if os.path.exists(jsonl_path):
    print(f'ğŸ“‹ å¼•ç”¨æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡é‡å»º: {jsonl_path}')
    return

  print("ğŸ” å¼€å§‹è§£æå¼•ç”¨...")
  ref_lines = parse_references_block(markdown_text)

  if not ref_lines:
    print("âš ï¸  æœªæ‰¾åˆ°å¼•ç”¨å†…å®¹ï¼Œåˆ›å»ºç©ºæ–‡ä»¶")
    write_jsonl([], jsonl_path)
    print(f"ğŸ“‹ ç©ºå¼•ç”¨æ–‡ä»¶: {jsonl_path}")
    return

  ref_entries = group_reference_entries(ref_lines)
  print(f"ğŸ“ åˆ†ç»„åå¾—åˆ° {len(ref_entries)} ä¸ªå¼•ç”¨æ¡ç›®")

  if not ref_entries:
    print("âš ï¸  å¼•ç”¨æ¡ç›®ä¸ºç©ºï¼Œåˆ›å»ºç©ºæ–‡ä»¶")
    write_jsonl([], jsonl_path)
    print(f"ğŸ“‹ ç©ºå¼•ç”¨æ–‡ä»¶: {jsonl_path}")
    return

  items = build_reference_items(ref_entries)
  print(f"ğŸ”— è§£æå¾—åˆ° {len(items)} ä¸ªæœ‰æ•ˆå¼•ç”¨")

  # å»é‡ï¼šå®Œå…¨ç›¸åŒçš„æ¡ç›®åªä¿ç•™ä¸€ä»½
  if items:
    seen = set()
    deduped = []
    for it in items:
      key = json.dumps(it, sort_keys=True, ensure_ascii=False)
      if key in seen:
        continue
      seen.add(key)
      deduped.append(it)
    removed = len(items) - len(deduped)
    if removed > 0:
      print(f'ğŸ”„ å»é‡: ç§»é™¤ {removed} æ¡é‡å¤å¼•ç”¨ (åŸå§‹ {len(items)} -> ä¿ç•™ {len(deduped)})')
    items = deduped

  write_jsonl(items, jsonl_path)

  print("âœ… å¤„ç†å®Œæˆ!")
  print(f"ğŸ“„ Markdown: {md_path}")
  print(f"ğŸ“‹ å¼•ç”¨æ–‡ä»¶: {jsonl_path} ({len(items)} æ¡)")

  if not items:
    print("âš ï¸  è­¦å‘Š: æœªèƒ½è§£æåˆ°å¼•ç”¨å†…å®¹")
    print("ğŸ’¡ å»ºè®®æ£€æŸ¥é¡µé¢æ˜¯å¦åŒ…å« References æ®µè½æˆ–è°ƒæ•´è§£æç­–ç•¥")


if __name__ == '__main__':
  main()
