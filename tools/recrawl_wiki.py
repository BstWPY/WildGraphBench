#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ›¿æ¢å·²æŠ“å–çš„ Wiki æ–‡ç« ä¸ºçº¯ Markdown æ ¼å¼

åŠŸèƒ½:
  1. æ‰«ææŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰ Wiki æ–‡ç«  Markdown æ–‡ä»¶
  2. ä½¿ç”¨ Jina API (X-Return-Format: markdown) é‡æ–°æŠ“å–
  3. ä¿ç•™åŸæœ‰çš„ reference/ ç›®å½•ç»“æ„ä¸å˜

ä½¿ç”¨ç¤ºä¾‹:
  # å•ä¸ªç›®å½•
  python tools/recrawl_wiki.py \
    --wiki-dir ./corpus/culture/Marvel_Cinematic_Universe

  # æ‰¹é‡å¤„ç† CSV ä¸­çš„æ‰€æœ‰ç›®å½•
  python tools/recrawl_wiki.py \
    --csv ./Top_references.csv \
    --root-dir ./corpus

  # æ‰¹é‡å¤„ç†æŒ‡å®šåˆ†ç±»
  python tools/recrawl_wiki.py \
    --root-dir ./corpus \
    --category culture
"""

import argparse
import csv
import json
import os
import re
import time
from pathlib import Path
from typing import Optional, Dict, Any
from urllib.parse import urlparse, unquote

import requests


DEFAULT_API_KEY = os.environ.get('JINA_API_KEY', '')


def normalize_category(category: str) -> str:
    """æ ‡å‡†åŒ–åˆ†ç±»åç§° (ä¸ run_ref_scraper.sh ä¿æŒä¸€è‡´)"""
    return (
        category
        .replace(' ', '_')
        .replace('&', '_and_')
        .replace(',', '_')
        .lower()
    )


def url_to_slug_title(url: str) -> str:
    """ä» Wiki URL æ¨å¯¼æ ‡é¢˜å­—ç¬¦ä¸²"""
    path = urlparse(url).path
    last = path.rsplit('/', 1)[-1]
    last = unquote(last)
    title = last.replace('_', ' ')
    return title or 'page'


def slugify(text: str) -> str:
    """ç®€å•çš„ slugify å®ç° (ä¸ jina_scraping ä¿æŒä¸€è‡´)"""
    text = text.lower()
    text = re.sub(r'[^\w\s-]', '', text)
    text = re.sub(r'[-\s]+', '-', text)
    return text.strip('-')


def fetch_wiki_markdown(url: str, api_key: str, max_retries: int = 3) -> Optional[str]:
    """ä½¿ç”¨ Jina API æŠ“å– Wiki æ–‡ç« çš„ Markdown æ ¼å¼
    
    Args:
        url: Wiki æ–‡ç«  URL
        api_key: Jina API Key
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        
    Returns:
        Markdown å†…å®¹å­—ç¬¦ä¸²,å¤±è´¥è¿”å› None
    """
    if not api_key.startswith('Bearer '):
        api_key = f'Bearer {api_key}'
    
    headers = {
        'Authorization': api_key,
        'X-Return-Format': 'markdown',  # å…³é”®: åªè¿”å› markdown
    }
    
    jina_url = f'https://r.jina.ai/{url}'
    
    for attempt in range(1, max_retries + 1):
        try:
            print(f"  ğŸ”„ å°è¯•æŠ“å– (ç¬¬ {attempt}/{max_retries} æ¬¡)...")
            
            response = requests.get(
                jina_url,
                headers=headers,
                timeout=60
            )
            
            if response.status_code == 200:
                content = response.text.strip()
                # â˜… åœ¨è¿™é‡Œå…ˆæŠŠ "* - Wikipedia" é¡¶éƒ¨æ ‡é¢˜å¹²æ‰
                content = strip_wikipedia_title_header(content)

                if content and len(content) > 100:  # åŸºæœ¬å†…å®¹æ£€æŸ¥
                    print(f"  âœ… æŠ“å–æˆåŠŸ: {len(content)} å­—ç¬¦ (æ¸…æ´—å)")
                    return content
                else:
                    print(f"  âš ï¸  è¿”å›å†…å®¹è¿‡çŸ­(æ¸…æ´—å): {len(content)} å­—ç¬¦")
                        
        except requests.exceptions.Timeout:
            print(f"  â±ï¸  è¯·æ±‚è¶…æ—¶")
        except Exception as e:
            print(f"  âŒ é”™è¯¯: {e}")
        
        if attempt < max_retries:
            wait_time = attempt * 5
            print(f"  â³ ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
            time.sleep(wait_time)
    
    return None


def find_wiki_markdown(wiki_dir: Path) -> Optional[Path]:
    """åœ¨ç›®å½•ä¸­æŸ¥æ‰¾ Wiki æ–‡ç« çš„ Markdown æ–‡ä»¶
    
    æŸ¥æ‰¾è§„åˆ™:
      1. æ–‡ä»¶åä¸åŒ…å« 'reference'
      2. æ–‡ä»¶æ‰©å±•åä¸º .md
      3. ä¸åœ¨ reference/ å­ç›®å½•ä¸‹
    
    Returns:
        Wiki Markdown æ–‡ä»¶è·¯å¾„,æœªæ‰¾åˆ°è¿”å› None
    """
    if not wiki_dir.exists() or not wiki_dir.is_dir():
        return None
    
    for md_file in wiki_dir.glob('*.md'):
        # æ’é™¤ reference ç›¸å…³æ–‡ä»¶
        if 'reference' in md_file.name.lower():
            continue
        
        # ç¡®ä¿ä¸åœ¨ reference å­ç›®å½•
        if 'reference' in str(md_file.relative_to(wiki_dir)).lower():
            continue
        
        return md_file
    
    return None


def extract_original_url(md_content: str) -> Optional[str]:
    """ä» Markdown å†…å®¹ä¸­æå–åŸå§‹ Wiki URL
    
    å°è¯•ä»ä»¥ä¸‹ä½ç½®æå–:
      1. Markdown metadata ä¸­çš„ URL å­—æ®µ
      2. æ–‡ä»¶å¼€å¤´çš„æ³¨é‡Š
      3. Title æ¨å¯¼ (æœ€åæ‰‹æ®µ)
    """
    # æ–¹æ³•1: æŸ¥æ‰¾ metadata
    url_match = re.search(r'URL Source:\s*(https?://[^\s\)]+)', md_content, re.IGNORECASE)
    if url_match:
        return url_match.group(1).strip()
    
    # æ–¹æ³•2: æŸ¥æ‰¾æ³¨é‡Š
    comment_match = re.search(r'<!--.*?(https://en\.wikipedia\.org/wiki/[^\s\)]+).*?-->', md_content, re.DOTALL)
    if comment_match:
        return comment_match.group(1).strip()
    
    # æ–¹æ³•3: ä»æ ‡é¢˜æ¨å¯¼ (ä¸å¤Ÿå‡†ç¡®,ä½†ä½œä¸ºå¤‡é€‰)
    title_match = re.search(r'^#\s+(.+)$', md_content, re.MULTILINE)
    if title_match:
        title = title_match.group(1).strip()
        # ç®€å•è½¬æ¢ä¸º Wiki URL æ ¼å¼
        wiki_slug = title.replace(' ', '_')
        return f"https://en.wikipedia.org/wiki/{wiki_slug}"
    
    return None


def replace_wiki_article(
    wiki_dir: Path,
    api_key: str,
    force: bool = False,
    backup: bool = True
) -> bool:
    """æ›¿æ¢å•ä¸ª Wiki ç›®å½•ä¸­çš„æ–‡ç« 
    
    Args:
        wiki_dir: Wiki ç›®å½•è·¯å¾„
        api_key: Jina API Key
        force: æ˜¯å¦å¼ºåˆ¶æ›¿æ¢ (å¿½ç•¥å¤‡ä»½æ£€æŸ¥)
        backup: æ˜¯å¦å¤‡ä»½åŸæ–‡ä»¶
        
    Returns:
        æ˜¯å¦æˆåŠŸæ›¿æ¢
    """
    print(f"\nğŸ“‚ å¤„ç†ç›®å½•: {wiki_dir}")
    
    # æŸ¥æ‰¾ Wiki Markdown æ–‡ä»¶
    md_file = find_wiki_markdown(wiki_dir)
    if not md_file:
        print("  âš ï¸  æœªæ‰¾åˆ° Wiki Markdown æ–‡ä»¶")
        return False
    
    print(f"  ğŸ“„ æ‰¾åˆ°æ–‡ä»¶: {md_file.name}")
    
    # è¯»å–åŸå†…å®¹æå– URL
    try:
        with open(md_file, 'r', encoding='utf-8') as f:
            original_content = f.read()
    except Exception as e:
        print(f"  âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return False
    
    original_url = extract_original_url(original_content)
    if not original_url:
        print("  âš ï¸  æ— æ³•ä»æ–‡ä»¶ä¸­æå–åŸå§‹ URL,è·³è¿‡")
        return False
    
    print(f"  ğŸ”— åŸå§‹ URL: {original_url}")
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰å¤‡ä»½
    backup_file = md_file.with_suffix('.md.bak')
    if backup_file.exists() and not force:
        print("  â„¹ï¸  å¤‡ä»½æ–‡ä»¶å·²å­˜åœ¨,è·³è¿‡ (ä½¿ç”¨ --force å¼ºåˆ¶æ›¿æ¢)")
        return False
    
    # æŠ“å–æ–°å†…å®¹
    new_content = fetch_wiki_markdown(original_url, api_key)
    if not new_content:
        print("  âŒ æŠ“å–å¤±è´¥")
        return False
    
    # å¤‡ä»½åŸæ–‡ä»¶
    if backup:
        try:
            backup_file.write_text(original_content, encoding='utf-8')
            print(f"  ğŸ’¾ å¤‡ä»½åŸæ–‡ä»¶: {backup_file.name}")
        except Exception as e:
            print(f"  âš ï¸  å¤‡ä»½å¤±è´¥: {e}")
    
    # å†™å…¥æ–°å†…å®¹
    try:
        md_file.write_text(new_content, encoding='utf-8')
        print(f"  âœ… æ›¿æ¢æˆåŠŸ: {len(new_content)} å­—ç¬¦")
        return True
    except Exception as e:
        print(f"  âŒ å†™å…¥å¤±è´¥: {e}")
        # å°è¯•æ¢å¤å¤‡ä»½
        if backup and backup_file.exists():
            try:
                md_file.write_text(original_content, encoding='utf-8')
                print("  ğŸ”„ å·²ä»å¤‡ä»½æ¢å¤")
            except:
                pass
        return False


def process_csv(
    csv_path: Path,
    root_dir: Path,
    api_key: str,
    start_row: int = 2,
    end_row: Optional[int] = None,
    force: bool = False,
    backup: bool = True
) -> Dict[str, int]:
    """æ‰¹é‡å¤„ç† CSV ä¸­çš„ Wiki ç›®å½•
    
    Returns:
        ç»Ÿè®¡ä¿¡æ¯å­—å…¸ {'total': æ€»æ•°, 'success': æˆåŠŸæ•°, 'failed': å¤±è´¥æ•°, 'skipped': è·³è¿‡æ•°}
    """
    stats = {'total': 0, 'success': 0, 'failed': 0, 'skipped': 0}
    
    with open(csv_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        rows = list(reader)
    
    if end_row is None:
        end_row = len(rows) + 1
    
    print(f"ğŸ“‹ CSV æ–‡ä»¶: {csv_path}")
    print(f"ğŸ“Š å¤„ç†èŒƒå›´: ç¬¬ {start_row} åˆ° {end_row} è¡Œ")
    
    for idx, row in enumerate(rows, start=2):  # CSV ç¬¬1è¡Œæ˜¯æ ‡é¢˜,ä»ç¬¬2è¡Œå¼€å§‹
        if idx < start_row or idx > end_row:
            continue
        
        stats['total'] += 1
        
        category = row.get('Category', '').strip()
        title = row.get('Title', '').strip()
        url = row.get('URL', '').strip()
        
        if not url:
            print(f"\n[{idx}] âš ï¸  URL ä¸ºç©º,è·³è¿‡")
            stats['skipped'] += 1
            continue
        
        # æ„å»ºç›®å½•è·¯å¾„
        category_dir = normalize_category(category)
        
        # â˜… ä¿®æ”¹ï¼šç›´æ¥ä½¿ç”¨ Title (ä¿ç•™ç©ºæ ¼)
        # å®é™…ç›®å½•åå¦‚: "Authoritarian socialism", "Steam service"
        wiki_dir = root_dir / category_dir / title
        
        print(f"\n{'='*60}")
        print(f"[{idx}/{len(rows) + 1}] å¤„ç†: {title}")
        print(f"{'='*60}")
        print(f"  ğŸ—‚ï¸  ç›®å½•: {wiki_dir}")
        
        if not wiki_dir.exists():
            print(f"  âš ï¸  ç›®å½•ä¸å­˜åœ¨")
            stats['skipped'] += 1
            continue
        
        if replace_wiki_article(wiki_dir, api_key, force, backup):
            stats['success'] += 1
        else:
            stats['failed'] += 1
        
        # é¿å…è¯·æ±‚è¿‡å¿«
        time.sleep(2)
    
    return stats


def process_category(
    root_dir: Path,
    category: str,
    api_key: str,
    force: bool = False,
    backup: bool = True
) -> Dict[str, int]:
    """æ‰¹é‡å¤„ç†æŒ‡å®šåˆ†ç±»ä¸‹çš„æ‰€æœ‰ Wiki ç›®å½•"""
    stats = {'total': 0, 'success': 0, 'failed': 0, 'skipped': 0}
    
    category_dir = root_dir / normalize_category(category)
    if not category_dir.exists():
        print(f"âŒ åˆ†ç±»ç›®å½•ä¸å­˜åœ¨: {category_dir}")
        return stats
    
    print(f"ğŸ“‚ å¤„ç†åˆ†ç±»: {category}")
    print(f"ğŸ“ ç›®å½•: {category_dir}")
    
    # éå†æ‰€æœ‰å­ç›®å½•
    for wiki_dir in sorted(category_dir.iterdir()):
        if not wiki_dir.is_dir():
            continue
        
        # è·³è¿‡ reference ç›¸å…³ç›®å½•
        if 'reference' in wiki_dir.name.lower():
            continue
        
        stats['total'] += 1
        
        if replace_wiki_article(wiki_dir, api_key, force, backup):
            stats['success'] += 1
        else:
            stats['failed'] += 1
        
        time.sleep(2)
    
    return stats


def main():
    parser = argparse.ArgumentParser(
        description='æ›¿æ¢å·²æŠ“å–çš„ Wiki æ–‡ç« ä¸ºçº¯ Markdown æ ¼å¼'
    )
    
    # è¾“å…¥æ¨¡å¼é€‰æ‹©
    input_group = parser.add_mutually_exclusive_group(required=True)
    input_group.add_argument(
        '--wiki-dir',
        type=Path,
        help='å•ä¸ª Wiki ç›®å½•è·¯å¾„'
    )
    input_group.add_argument(
        '--csv',
        type=Path,
        help='CSV æ–‡ä»¶è·¯å¾„ (æ‰¹é‡å¤„ç†)'
    )
    input_group.add_argument(
        '--category',
        type=str,
        help='åˆ†ç±»åç§° (éœ€é…åˆ --root-dir)'
    )
    
    # é€šç”¨å‚æ•°
    parser.add_argument(
        '--root-dir',
        type=Path,
        default=Path('./corpus'),
        help='æ ¹ç›®å½• (ç”¨äº CSV/åˆ†ç±»æ¨¡å¼)'
    )
    parser.add_argument(
        '--api-key',
        default=None,
        help='Jina API Key (or set JINA_API_KEY environment variable)'
    )
    parser.add_argument(
        '--start',
        type=int,
        default=2,
        help='CSV èµ·å§‹è¡Œ (1-based, é»˜è®¤2è·³è¿‡æ ‡é¢˜)'
    )
    parser.add_argument(
        '--end',
        type=int,
        default=None,
        help='CSV ç»“æŸè¡Œ (1-based, åŒ…å«)'
    )
    parser.add_argument(
        '--force',
        action='store_true',
        help='å¼ºåˆ¶æ›¿æ¢ (å¿½ç•¥å·²å­˜åœ¨çš„å¤‡ä»½)'
    )
    parser.add_argument(
        '--no-backup',
        action='store_true',
        help='ä¸å¤‡ä»½åŸæ–‡ä»¶'
    )
    
    args = parser.parse_args()
    
    # API Key
    api_key = args.api_key or os.environ.get('JINA_API_KEY') or DEFAULT_API_KEY
    
    print("ğŸš€ Wiki Markdown æ›¿æ¢å·¥å…·")
    print(f"ğŸ”‘ API Key: {api_key[:20]}...")
    print(f"ğŸ’¾ å¤‡ä»½: {'å¦' if args.no_backup else 'æ˜¯'}")
    print(f"ğŸ”„ å¼ºåˆ¶: {'æ˜¯' if args.force else 'å¦'}")
    
    # å•ç›®å½•æ¨¡å¼
    if args.wiki_dir:
        success = replace_wiki_article(
            args.wiki_dir,
            api_key,
            force=args.force,
            backup=not args.no_backup
        )
        print(f"\n{'='*60}")
        print(f"âœ… å®Œæˆ" if success else "âŒ å¤±è´¥")
        return
    
    # CSV æ‰¹é‡æ¨¡å¼
    if args.csv:
        stats = process_csv(
            args.csv,
            args.root_dir,
            api_key,
            start_row=args.start,
            end_row=args.end,
            force=args.force,
            backup=not args.no_backup
        )
    
    # åˆ†ç±»æ‰¹é‡æ¨¡å¼
    elif args.category:
        stats = process_category(
            args.root_dir,
            args.category,
            api_key,
            force=args.force,
            backup=not args.no_backup
        )
    
    else:
        parser.error("éœ€è¦æŒ‡å®š --wiki-dir, --csv æˆ– --category")
        return
    
    # æ‰“å°ç»Ÿè®¡
    print(f"\n{'='*60}")
    print("ğŸ“Š å¤„ç†ç»Ÿè®¡")
    print(f"{'='*60}")
    print(f"  æ€»è®¡: {stats['total']}")
    print(f"  âœ… æˆåŠŸ: {stats['success']}")
    print(f"  âŒ å¤±è´¥: {stats['failed']}")
    print(f"  â­ï¸  è·³è¿‡: {stats['skipped']}")


if __name__ == '__main__':
    main()